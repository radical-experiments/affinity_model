                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.45#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.45#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.45#

               Core t (s)   Wall t (s)        (%)
       Time:      846.647      846.647      100.0
                 (ns/day)    (hour/ns)
Performance:        5.103        4.703

GROMACS reminds you: "If mathematical analysis should ever hold a prominent place in chemistry - an aberration which is happily almost impossible - it would occasion a rapid and widespread degeneration of that science." (Aguste Comte, 1830)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     846547.293005      task-clock (msec)         #    1.000 CPUs utilized          
            86,660      context-switches          #    0.102 K/sec                  
                26      cpu-migrations            #    0.000 K/sec                  
            74,664      page-faults               #    0.088 K/sec                  
 2,784,644,905,559      cycles                    #    3.289 GHz                    
 1,488,369,376,646      stalled-cycles-frontend   #   53.45% frontend cycles idle   
   265,775,884,502      stalled-cycles-backend    #    9.54% backend cycles idle    
 4,576,974,169,652      instructions              #    1.64  insn per cycle         
                                                  #    0.33  stalled cycles per insn
   124,983,155,722      branches                  #  147.639 M/sec                  
     1,841,500,430      branch-misses             #    1.47% of all branches        

     846.798584304 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.46#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.46#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.46#

               Core t (s)   Wall t (s)        (%)
       Time:      846.029      846.029      100.0
                 (ns/day)    (hour/ns)
Performance:        5.106        4.700

GROMACS reminds you: "Take Dehydrated Water On Your Desert Trips" (Space Quest III)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     845931.813231      task-clock (msec)         #    1.000 CPUs utilized          
            86,595      context-switches          #    0.102 K/sec                  
                20      cpu-migrations            #    0.000 K/sec                  
            74,654      page-faults               #    0.088 K/sec                  
 2,782,755,240,532      cycles                    #    3.290 GHz                    
 1,486,007,392,575      stalled-cycles-frontend   #   53.40% frontend cycles idle   
   265,320,168,691      stalled-cycles-backend    #    9.53% backend cycles idle    
 4,577,615,051,280      instructions              #    1.64  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,990,615,307      branches                  #  147.755 M/sec                  
     1,846,825,183      branch-misses             #    1.48% of all branches        

     846.180506857 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.47#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.47#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.47#

               Core t (s)   Wall t (s)        (%)
       Time:      843.727      843.727      100.0
                 (ns/day)    (hour/ns)
Performance:        5.120        4.687

GROMACS reminds you: "I Am Testing Your Grey Matter" (Red Hot Chili Peppers)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     843616.610976      task-clock (msec)         #    1.000 CPUs utilized          
            86,360      context-switches          #    0.102 K/sec                  
                23      cpu-migrations            #    0.000 K/sec                  
            54,852      page-faults               #    0.065 K/sec                  
 2,774,929,114,335      cycles                    #    3.289 GHz                    
 1,478,524,637,024      stalled-cycles-frontend   #   53.28% frontend cycles idle   
   259,324,555,388      stalled-cycles-backend    #    9.35% backend cycles idle    
 4,576,960,830,868      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,977,773,020      branches                  #  148.145 M/sec                  
     1,839,110,649      branch-misses             #    1.47% of all branches        

     843.873396019 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.48#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.48#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.48#

               Core t (s)   Wall t (s)        (%)
       Time:      852.335      852.335      100.0
                 (ns/day)    (hour/ns)
Performance:        5.069        4.735

GROMACS reminds you: "Suzy is a headbanger, her mother is a geek" (The Ramones)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     852223.939753      task-clock (msec)         #    1.000 CPUs utilized          
            87,266      context-switches          #    0.102 K/sec                  
                18      cpu-migrations            #    0.000 K/sec                  
            75,372      page-faults               #    0.088 K/sec                  
 2,803,278,395,530      cycles                    #    3.289 GHz                    
 1,504,906,272,733      stalled-cycles-frontend   #   53.68% frontend cycles idle   
   276,819,791,912      stalled-cycles-backend    #    9.87% backend cycles idle    
 4,577,676,781,315      instructions              #    1.63  insn per cycle         
                                                  #    0.33  stalled cycles per insn
   124,990,363,543      branches                  #  146.664 M/sec                  
     2,128,853,769      branch-misses             #    1.70% of all branches        

     852.482017471 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.49#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.49#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.49#

               Core t (s)   Wall t (s)        (%)
       Time:      989.968      989.968      100.0
                 (ns/day)    (hour/ns)
Performance:        4.364        5.500

GROMACS reminds you: "Take Your Medications and Preparations and Ram It Up Your Snout" (F. Zappa)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     989838.165951      task-clock (msec)         #    1.000 CPUs utilized          
           101,266      context-switches          #    0.102 K/sec                  
                22      cpu-migrations            #    0.000 K/sec                  
            81,167      page-faults               #    0.082 K/sec                  
 3,255,768,391,888      cycles                    #    3.289 GHz                    
 1,960,129,026,304      stalled-cycles-frontend   #   60.20% frontend cycles idle   
   520,033,850,704      stalled-cycles-backend    #   15.97% backend cycles idle    
 4,577,397,610,036      instructions              #    1.41  insn per cycle         
                                                  #    0.43  stalled cycles per insn
   125,071,286,592      branches                  #  126.355 M/sec                  
     1,840,981,763      branch-misses             #    1.47% of all branches        

     990.120293572 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.50#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.50#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.50#

               Core t (s)   Wall t (s)        (%)
       Time:      862.030      862.030      100.0
                 (ns/day)    (hour/ns)
Performance:        5.012        4.789

GROMACS reminds you: "I Live the Life They Wish They Did" (Tricky)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     861924.951149      task-clock (msec)         #    1.000 CPUs utilized          
            88,226      context-switches          #    0.102 K/sec                  
                20      cpu-migrations            #    0.000 K/sec                  
            55,589      page-faults               #    0.064 K/sec                  
 2,834,807,726,195      cycles                    #    3.289 GHz                    
 1,538,055,131,743      stalled-cycles-frontend   #   54.26% frontend cycles idle   
   291,736,867,647      stalled-cycles-backend    #   10.29% backend cycles idle    
 4,577,714,649,018      instructions              #    1.61  insn per cycle         
                                                  #    0.34  stalled cycles per insn
   124,991,674,480      branches                  #  145.015 M/sec                  
     1,839,125,529      branch-misses             #    1.47% of all branches        

     862.178127138 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.51#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.51#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.51#

               Core t (s)   Wall t (s)        (%)
       Time:      846.499      846.499      100.0
                 (ns/day)    (hour/ns)
Performance:        5.104        4.703

GROMACS reminds you: "People disagree with me. I just ignore them." (Linus Torvalds on the use of C++ in the kernel)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     846374.042324      task-clock (msec)         #    1.000 CPUs utilized          
            86,642      context-switches          #    0.102 K/sec                  
                15      cpu-migrations            #    0.000 K/sec                  
            74,649      page-faults               #    0.088 K/sec                  
 2,783,733,359,531      cycles                    #    3.289 GHz                    
 1,487,082,211,132      stalled-cycles-frontend   #   53.42% frontend cycles idle   
   266,378,050,502      stalled-cycles-backend    #    9.57% backend cycles idle    
 4,576,484,061,775      instructions              #    1.64  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,985,267,177      branches                  #  147.671 M/sec                  
     1,846,448,514      branch-misses             #    1.48% of all branches        

     846.649370739 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.52#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.52#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.52#

               Core t (s)   Wall t (s)        (%)
       Time:      852.598      852.598      100.0
                 (ns/day)    (hour/ns)
Performance:        5.067        4.736

GROMACS reminds you: "Der Ball ist rund, das Spiel dauert 90 minuten, alles andere ist Theorie" (Lola rennt)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     852478.384367      task-clock (msec)         #    1.000 CPUs utilized          
            87,288      context-switches          #    0.102 K/sec                  
                24      cpu-migrations            #    0.000 K/sec                  
            75,600      page-faults               #    0.089 K/sec                  
 2,804,051,346,908      cycles                    #    3.289 GHz                    
 1,507,371,781,513      stalled-cycles-frontend   #   53.76% frontend cycles idle   
   276,333,807,777      stalled-cycles-backend    #    9.85% backend cycles idle    
 4,577,735,730,662      instructions              #    1.63  insn per cycle         
                                                  #    0.33  stalled cycles per insn
   124,998,238,989      branches                  #  146.629 M/sec                  
     1,838,580,719      branch-misses             #    1.47% of all branches        

     852.747853743 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.53#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.53#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.53#

               Core t (s)   Wall t (s)        (%)
       Time:      845.424      845.424      100.0
                 (ns/day)    (hour/ns)
Performance:        5.110        4.697

GROMACS reminds you: "You Own the Sun" (Throwing Muses)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     845338.050510      task-clock (msec)         #    1.000 CPUs utilized          
            86,537      context-switches          #    0.102 K/sec                  
                22      cpu-migrations            #    0.000 K/sec                  
            62,547      page-faults               #    0.074 K/sec                  
 2,780,356,970,762      cycles                    #    3.289 GHz                    
 1,483,809,006,691      stalled-cycles-frontend   #   53.37% frontend cycles idle   
   265,321,834,481      stalled-cycles-backend    #    9.54% backend cycles idle    
 4,576,532,289,886      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,970,524,294      branches                  #  147.835 M/sec                  
     1,851,524,406      branch-misses             #    1.48% of all branches        

     845.575515983 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.54#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.54#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.54#

               Core t (s)   Wall t (s)        (%)
       Time:      846.410      846.410      100.0
                 (ns/day)    (hour/ns)
Performance:        5.104        4.702

GROMACS reminds you: "There's only music to make new ringtones" (Arctic Monkeys)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     846312.379238      task-clock (msec)         #    1.000 CPUs utilized          
            86,633      context-switches          #    0.102 K/sec                  
                18      cpu-migrations            #    0.000 K/sec                  
            75,101      page-faults               #    0.089 K/sec                  
 2,783,727,592,032      cycles                    #    3.289 GHz                    
 1,487,139,760,191      stalled-cycles-frontend   #   53.42% frontend cycles idle   
   262,883,027,252      stalled-cycles-backend    #    9.44% backend cycles idle    
 4,576,478,258,018      instructions              #    1.64  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,980,798,905      branches                  #  147.677 M/sec                  
     1,845,014,940      branch-misses             #    1.48% of all branches        

     846.560384025 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.55#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.55#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.55#

               Core t (s)   Wall t (s)        (%)
       Time:      917.058      917.058      100.0
                 (ns/day)    (hour/ns)
Performance:        4.711        5.095

GROMACS reminds you: "Physics is a few rules, and with some handwaving you can make up the rest" (Michael Levitt)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     916931.443633      task-clock (msec)         #    1.000 CPUs utilized          
            93,839      context-switches          #    0.102 K/sec                  
                31      cpu-migrations            #    0.000 K/sec                  
            78,376      page-faults               #    0.085 K/sec                  
 3,016,081,896,151      cycles                    #    3.289 GHz                    
 1,719,642,495,052      stalled-cycles-frontend   #   57.02% frontend cycles idle   
   373,059,461,623      stalled-cycles-backend    #   12.37% backend cycles idle    
 4,576,723,998,963      instructions              #    1.52  insn per cycle         
                                                  #    0.38  stalled cycles per insn
   125,024,961,401      branches                  #  136.351 M/sec                  
     1,847,017,417      branch-misses             #    1.48% of all branches        

     917.209028339 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.56#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.56#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.56#

               Core t (s)   Wall t (s)        (%)
       Time:      915.751      915.751      100.0
                 (ns/day)    (hour/ns)
Performance:        4.718        5.087

GROMACS reminds you: "As an adolescent I aspired to lasting fame, I craved factual certainty, and I thirsted for a meaningful vision of human life -- so I became a scientist. This is like becoming an archbishop so you can meet girls." (Matt Cartmill)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     915625.445428      task-clock (msec)         #    1.000 CPUs utilized          
            93,708      context-switches          #    0.102 K/sec                  
                22      cpu-migrations            #    0.000 K/sec                  
            66,061      page-faults               #    0.072 K/sec                  
 3,011,912,422,138      cycles                    #    3.289 GHz                    
 1,715,072,337,129      stalled-cycles-frontend   #   56.94% frontend cycles idle   
   369,740,156,332      stalled-cycles-backend    #   12.28% backend cycles idle    
 4,578,183,377,752      instructions              #    1.52  insn per cycle         
                                                  #    0.37  stalled cycles per insn
   125,033,730,069      branches                  #  136.556 M/sec                  
     1,843,741,920      branch-misses             #    1.47% of all branches        

     915.899965487 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.57#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.57#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.57#

               Core t (s)   Wall t (s)        (%)
       Time:      863.869      863.869      100.0
                 (ns/day)    (hour/ns)
Performance:        5.001        4.799

GROMACS reminds you: "Ich Bin Ein Berliner" (J.F. Kennedy)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     863777.950954      task-clock (msec)         #    1.000 CPUs utilized          
            88,404      context-switches          #    0.102 K/sec                  
                20      cpu-migrations            #    0.000 K/sec                  
            75,173      page-faults               #    0.087 K/sec                  
 2,841,235,490,946      cycles                    #    3.289 GHz                    
 1,544,448,074,968      stalled-cycles-frontend   #   54.36% frontend cycles idle   
   284,096,345,843      stalled-cycles-backend    #   10.00% backend cycles idle    
 4,577,229,856,490      instructions              #    1.61  insn per cycle         
                                                  #    0.34  stalled cycles per insn
   124,993,134,096      branches                  #  144.705 M/sec                  
     1,844,364,865      branch-misses             #    1.48% of all branches        

     864.018059684 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.58#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.58#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.58#

               Core t (s)   Wall t (s)        (%)
       Time:      990.099      990.099      100.0
                 (ns/day)    (hour/ns)
Performance:        4.363        5.500

GROMACS reminds you: "The Poodle Chews It" (F. Zappa)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     989964.733790      task-clock (msec)         #    1.000 CPUs utilized          
           101,309      context-switches          #    0.102 K/sec                  
                29      cpu-migrations            #    0.000 K/sec                  
            81,567      page-faults               #    0.082 K/sec                  
 3,256,039,255,366      cycles                    #    3.289 GHz                    
 1,960,346,977,552      stalled-cycles-frontend   #   60.21% frontend cycles idle   
   520,292,014,219      stalled-cycles-backend    #   15.98% backend cycles idle    
 4,577,818,661,284      instructions              #    1.41  insn per cycle         
                                                  #    0.43  stalled cycles per insn
   125,086,260,944      branches                  #  126.354 M/sec                  
     1,840,711,672      branch-misses             #    1.47% of all branches        

     990.248197257 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.59#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.59#
starting mdrun 'Protein in water'
25000 steps,     50.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.59#

               Core t (s)   Wall t (s)        (%)
       Time:      845.318      845.318      100.0
                 (ns/day)    (hour/ns)
Performance:        5.111        4.696

GROMACS reminds you: "Wicky-wicky Wa-wild West" (Will Smith)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     845209.503926      task-clock (msec)         #    1.000 CPUs utilized          
            86,524      context-switches          #    0.102 K/sec                  
                16      cpu-migrations            #    0.000 K/sec                  
            62,855      page-faults               #    0.074 K/sec                  
 2,779,753,454,675      cycles                    #    3.289 GHz                    
 1,483,318,816,023      stalled-cycles-frontend   #   53.36% frontend cycles idle   
   263,121,315,287      stalled-cycles-backend    #    9.47% backend cycles idle    
 4,576,898,810,817      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
   124,979,287,459      branches                  #  147.868 M/sec                  
     1,845,177,090      branch-misses             #    1.48% of all branches        

     845.467362552 seconds time elapsed

