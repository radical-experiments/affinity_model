                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.60#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.60#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.60#

               Core t (s)   Wall t (s)        (%)
       Time:     1617.309     1617.309      100.0
                 (ns/day)    (hour/ns)
Performance:        5.342        4.492

GROMACS reminds you: "Beware of bugs in the above code; I have only proved it correct, not tried it." (Donald Knuth)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1618911.868860      task-clock (msec)         #    1.001 CPUs utilized          
             8,214      context-switches          #    0.005 K/sec                  
                 9      cpu-migrations            #    0.000 K/sec                  
            12,517      page-faults               #    0.008 K/sec                  
 4,521,851,900,227      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,548,324,857,350      instructions              #    2.33  insns per cycle        
   158,970,545,336      branches                  #   98.196 M/sec                  
     2,842,444,589      branch-misses             #    1.79% of all branches        

    1617.687244687 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.61#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.61#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.61#

               Core t (s)   Wall t (s)        (%)
       Time:     1618.630     1618.630      100.0
                 (ns/day)    (hour/ns)
Performance:        5.338        4.496

GROMACS reminds you: "If at one time or another I have brushed a few colleagues the wrong way, I must apologize: I had not realized that they were covered with fur." (Edwin Chargaff)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1620232.074756      task-clock (msec)         #    1.001 CPUs utilized          
             8,159      context-switches          #    0.005 K/sec                  
                14      cpu-migrations            #    0.000 K/sec                  
            12,518      page-faults               #    0.008 K/sec                  
 4,525,474,280,211      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,550,153,678,454      instructions              #    2.33  insns per cycle        
   159,030,421,745      branches                  #   98.153 M/sec                  
     2,873,159,133      branch-misses             #    1.81% of all branches        

    1619.016299693 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.62#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.62#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.62#

               Core t (s)   Wall t (s)        (%)
       Time:     1597.924     1597.924      100.0
                 (ns/day)    (hour/ns)
Performance:        5.407        4.439

GROMACS reminds you: "I managed to get two hours of work done before work" (E. Lindahl)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1599505.647301      task-clock (msec)         #    1.001 CPUs utilized          
             8,191      context-switches          #    0.005 K/sec                  
                30      cpu-migrations            #    0.000 K/sec                  
            12,647      page-faults               #    0.008 K/sec                  
 4,467,644,652,281      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,550,784,318,373      instructions              #    2.36  insns per cycle        
   159,017,147,201      branches                  #   99.416 M/sec                  
     2,868,571,540      branch-misses             #    1.80% of all branches        

    1598.309079791 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.63#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.63#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.63#

               Core t (s)   Wall t (s)        (%)
       Time:     1601.414     1601.414      100.0
                 (ns/day)    (hour/ns)
Performance:        5.395        4.448

GROMACS reminds you: "Miggida-Miggida-Miggida-Mac" (Kriss Kross)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1603006.370721      task-clock (msec)         #    1.001 CPUs utilized          
             8,107      context-switches          #    0.005 K/sec                  
                17      cpu-migrations            #    0.000 K/sec                  
            12,517      page-faults               #    0.008 K/sec                  
 4,477,456,505,927      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,551,354,647,828      instructions              #    2.36  insns per cycle        
   159,092,355,373      branches                  #   99.246 M/sec                  
     2,866,072,260      branch-misses             #    1.80% of all branches        

    1601.797222081 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.64#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.64#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.64#

               Core t (s)   Wall t (s)        (%)
       Time:     1600.110     1600.110      100.0
                 (ns/day)    (hour/ns)
Performance:        5.400        4.445

GROMACS reminds you: "The easiest way to scale well is to have bad single-core performance" (Blind Freddie)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1601699.031385      task-clock (msec)         #    1.001 CPUs utilized          
             8,222      context-switches          #    0.005 K/sec                  
                 6      cpu-migrations            #    0.000 K/sec                  
            12,654      page-faults               #    0.008 K/sec                  
 4,473,768,530,210      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,554,551,762,289      instructions              #    2.36  insns per cycle        
   159,245,705,640      branches                  #   99.423 M/sec                  
     2,875,273,101      branch-misses             #    1.81% of all branches        

    1600.492582928 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.65#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.65#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.65#

               Core t (s)   Wall t (s)        (%)
       Time:     1610.266     1610.266      100.0
                 (ns/day)    (hour/ns)
Performance:        5.366        4.473

GROMACS reminds you: "We mathematicians are all a bit crazy." (Lev Landau)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1611863.788996      task-clock (msec)         #    1.001 CPUs utilized          
             8,191      context-switches          #    0.005 K/sec                  
                16      cpu-migrations            #    0.000 K/sec                  
            12,516      page-faults               #    0.008 K/sec                  
 4,502,116,966,454      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,552,021,185,269      instructions              #    2.34  insns per cycle        
   159,029,189,959      branches                  #   98.662 M/sec                  
     2,869,280,488      branch-misses             #    1.80% of all branches        

    1610.645243197 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.66#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.66#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.66#

               Core t (s)   Wall t (s)        (%)
       Time:     1600.816     1600.816      100.0
                 (ns/day)    (hour/ns)
Performance:        5.397        4.447

GROMACS reminds you: "There is just one thing I can promise you about the outer-space program: your tax dollar will go farther." (Wernher von Braun)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1602404.141348      task-clock (msec)         #    1.001 CPUs utilized          
             8,144      context-switches          #    0.005 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            12,653      page-faults               #    0.008 K/sec                  
 4,475,712,981,840      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,550,649,095,307      instructions              #    2.36  insns per cycle        
   159,020,890,743      branches                  #   99.239 M/sec                  
     2,869,036,848      branch-misses             #    1.80% of all branches        

    1601.202208264 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.67#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.67#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.67#

               Core t (s)   Wall t (s)        (%)
       Time:     1597.670     1597.670      100.0
                 (ns/day)    (hour/ns)
Performance:        5.408        4.438

GROMACS reminds you: "Unfortunately, "simulation" has become increasingly misused to mean nothing more than "calculation"" (Bill Jorgensen)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1599252.798604      task-clock (msec)         #    1.001 CPUs utilized          
             8,234      context-switches          #    0.005 K/sec                  
                13      cpu-migrations            #    0.000 K/sec                  
            12,518      page-faults               #    0.008 K/sec                  
 4,466,950,106,790      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,552,208,798,775      instructions              #    2.36  insns per cycle        
   159,088,308,873      branches                  #   99.477 M/sec                  
     2,863,771,967      branch-misses             #    1.80% of all branches        

    1598.057135773 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.68#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.68#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.68#

               Core t (s)   Wall t (s)        (%)
       Time:     1599.556     1599.556      100.0
                 (ns/day)    (hour/ns)
Performance:        5.402        4.443

GROMACS reminds you: "Sometimes Life is Obscene" (Black Crowes)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1601143.261727      task-clock (msec)         #    1.001 CPUs utilized          
             8,175      context-switches          #    0.005 K/sec                  
                 4      cpu-migrations            #    0.000 K/sec                  
            12,516      page-faults               #    0.008 K/sec                  
 4,472,215,063,740      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,552,218,919,517      instructions              #    2.36  insns per cycle        
   159,164,048,954      branches                  #   99.407 M/sec                  
     2,870,922,121      branch-misses             #    1.80% of all branches        

    1599.941338928 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.69#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.69#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.69#

               Core t (s)   Wall t (s)        (%)
       Time:     1609.694     1609.694      100.0
                 (ns/day)    (hour/ns)
Performance:        5.368        4.471

GROMACS reminds you: "I Wrapped a Newspaper Round My Head" (F. Zappa)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1611284.817613      task-clock (msec)         #    1.001 CPUs utilized          
             8,292      context-switches          #    0.005 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            12,515      page-faults               #    0.008 K/sec                  
 4,500,512,184,944      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,552,801,446,469      instructions              #    2.34  insns per cycle        
   159,234,409,342      branches                  #   98.824 M/sec                  
     2,868,125,324      branch-misses             #    1.80% of all branches        

    1610.082863783 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.70#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.70#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.70#

               Core t (s)   Wall t (s)        (%)
       Time:     1613.727     1613.727      100.0
                 (ns/day)    (hour/ns)
Performance:        5.354        4.482

GROMACS reminds you: "FORTRAN was the language of choice for the same reason that three-legged races are popular." (Ken Thompson)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1615322.167680      task-clock (msec)         #    1.001 CPUs utilized          
             8,185      context-switches          #    0.005 K/sec                  
                 6      cpu-migrations            #    0.000 K/sec                  
            12,516      page-faults               #    0.008 K/sec                  
 4,511,826,849,272      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,553,450,160,895      instructions              #    2.34  insns per cycle        
   159,100,891,531      branches                  #   98.495 M/sec                  
     2,870,052,622      branch-misses             #    1.80% of all branches        

    1614.117629685 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.71#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.71#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.71#

               Core t (s)   Wall t (s)        (%)
       Time:     1598.796     1598.796      100.0
                 (ns/day)    (hour/ns)
Performance:        5.404        4.441

GROMACS reminds you: "In science, truth always wins." (Max Perutz)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1600379.229554      task-clock (msec)         #    1.001 CPUs utilized          
             8,300      context-switches          #    0.005 K/sec                  
                 9      cpu-migrations            #    0.000 K/sec                  
            12,515      page-faults               #    0.008 K/sec                  
 4,470,009,774,115      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,550,045,750,536      instructions              #    2.36  insns per cycle        
   159,010,748,919      branches                  #   99.358 M/sec                  
     2,867,064,700      branch-misses             #    1.80% of all branches        

    1599.184830325 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.72#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.72#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.72#

               Core t (s)   Wall t (s)        (%)
       Time:     1609.734     1609.734      100.0
                 (ns/day)    (hour/ns)
Performance:        5.367        4.471

GROMACS reminds you: "'Nay. We are but men.' Rock!" (Tenacious D)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1611328.166838      task-clock (msec)         #    1.001 CPUs utilized          
             8,256      context-switches          #    0.005 K/sec                  
                 7      cpu-migrations            #    0.000 K/sec                  
            12,516      page-faults               #    0.008 K/sec                  
 4,500,586,154,512      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,550,412,759,230      instructions              #    2.34  insns per cycle        
   159,014,151,727      branches                  #   98.685 M/sec                  
     2,866,854,590      branch-misses             #    1.80% of all branches        

    1610.124274944 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.73#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.73#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.73#

               Core t (s)   Wall t (s)        (%)
       Time:     1610.460     1610.460      100.0
                 (ns/day)    (hour/ns)
Performance:        5.365        4.473

GROMACS reminds you: "I've Got Two Turntables and a Microphone" (B. Hansen)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1612058.140482      task-clock (msec)         #    1.001 CPUs utilized          
             8,340      context-switches          #    0.005 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            12,517      page-faults               #    0.008 K/sec                  
 4,502,636,870,598      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,552,318,146,457      instructions              #    2.34  insns per cycle        
   159,037,956,297      branches                  #   98.655 M/sec                  
     2,867,028,388      branch-misses             #    1.80% of all branches        

    1610.848178890 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.74#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-17-04.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.74#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.74#

               Core t (s)   Wall t (s)        (%)
       Time:     1612.650     1612.650      100.0
                 (ns/day)    (hour/ns)
Performance:        5.358        4.479

GROMACS reminds you: "If Java had true garbage collection, most programs would delete themselves upon execution." (Robert Sewell)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1614235.179124      task-clock (msec)         #    1.001 CPUs utilized          
             8,326      context-switches          #    0.005 K/sec                  
                 7      cpu-migrations            #    0.000 K/sec                  
            12,518      page-faults               #    0.008 K/sec                  
 4,508,741,321,353      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
10,553,284,820,034      instructions              #    2.34  insns per cycle        
   159,111,536,543      branches                  #   98.568 M/sec                  
     2,873,774,327      branch-misses             #    1.81% of all branches        

    1613.040398145 seconds time elapsed

