                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.15#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.15#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.15#

               Core t (s)   Wall t (s)        (%)
       Time:      160.507      160.507      100.0
                 (ns/day)    (hour/ns)
Performance:        5.384        4.458

GROMACS reminds you: "It Wouldn't Hurt to Wipe Once In a While" (Beavis and Butthead)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     160877.247939      task-clock (msec)         #    1.000 CPUs utilized          
             2,474      context-switches          #    0.015 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
            12,497      page-faults               #    0.078 K/sec                  
   449,377,039,396      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,146,345,338      instructions              #    2.35  insns per cycle        
    16,059,657,825      branches                  #   99.826 M/sec                  
       288,499,572      branch-misses             #    1.80% of all branches        

     160.857958393 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.16#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.16#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.16#

               Core t (s)   Wall t (s)        (%)
       Time:      160.610      160.610      100.0
                 (ns/day)    (hour/ns)
Performance:        5.381        4.461

GROMACS reminds you: "Lunatics On Pogo Sticks" (Red Hot Chili Peppers)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     160981.443198      task-clock (msec)         #    1.000 CPUs utilized          
             2,503      context-switches          #    0.016 K/sec                  
                 5      cpu-migrations            #    0.000 K/sec                  
            12,498      page-faults               #    0.078 K/sec                  
   449,667,232,642      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,133,534,303      instructions              #    2.35  insns per cycle        
    16,067,721,478      branches                  #   99.811 M/sec                  
       288,568,490      branch-misses             #    1.80% of all branches        

     160.964686985 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.17#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.17#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.17#

               Core t (s)   Wall t (s)        (%)
       Time:      158.399      158.399      100.0
                 (ns/day)    (hour/ns)
Performance:        5.456        4.399

GROMACS reminds you: "You always pass failure on the way to success." (Mickey Rooney)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     158768.286520      task-clock (msec)         #    1.000 CPUs utilized          
             2,477      context-switches          #    0.016 K/sec                  
                 1      cpu-migrations            #    0.000 K/sec                  
            12,498      page-faults               #    0.079 K/sec                  
   443,483,318,523      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,055,847,213,366      instructions              #    2.38  insns per cycle        
    16,063,785,684      branches                  #  101.178 M/sec                  
       287,976,702      branch-misses             #    1.79% of all branches        

     158.751887026 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.18#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.18#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.18#

               Core t (s)   Wall t (s)        (%)
       Time:      158.703      158.703      100.0
                 (ns/day)    (hour/ns)
Performance:        5.445        4.408

GROMACS reminds you: "Can I have everything louder than everything else?" (Deep Purple)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159071.345587      task-clock (msec)         #    1.000 CPUs utilized          
             2,496      context-switches          #    0.016 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
            12,498      page-faults               #    0.079 K/sec                  
   444,333,545,743      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,501,117,591      instructions              #    2.38  insns per cycle        
    16,083,197,753      branches                  #  101.107 M/sec                  
       288,212,971      branch-misses             #    1.79% of all branches        

     159.056979516 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.19#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.19#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.19#

               Core t (s)   Wall t (s)        (%)
       Time:      158.837      158.837      100.0
                 (ns/day)    (hour/ns)
Performance:        5.441        4.411

GROMACS reminds you: "My greatest contribution to the field of science is that I never entered it." (Colin Powell)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159203.554031      task-clock (msec)         #    1.000 CPUs utilized          
             2,514      context-switches          #    0.016 K/sec                  
                 7      cpu-migrations            #    0.000 K/sec                  
            12,499      page-faults               #    0.079 K/sec                  
   444,709,441,574      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,382,906,475      instructions              #    2.38  insns per cycle        
    16,060,486,762      branches                  #  100.880 M/sec                  
       288,441,324      branch-misses             #    1.80% of all branches        

     159.194730674 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.20#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.20#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.20#

               Core t (s)   Wall t (s)        (%)
       Time:      158.683      158.683      100.0
                 (ns/day)    (hour/ns)
Performance:        5.446        4.407

GROMACS reminds you: "A Man Needs a Maid" (N. Young)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159052.108946      task-clock (msec)         #    1.000 CPUs utilized          
             2,491      context-switches          #    0.016 K/sec                  
                 4      cpu-migrations            #    0.000 K/sec                  
            12,496      page-faults               #    0.079 K/sec                  
   444,276,844,620      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,822,304,217      instructions              #    2.38  insns per cycle        
    16,062,272,722      branches                  #  100.987 M/sec                  
       289,058,593      branch-misses             #    1.80% of all branches        

     159.037664811 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.21#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.21#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.21#

               Core t (s)   Wall t (s)        (%)
       Time:      160.146      160.146      100.0
                 (ns/day)    (hour/ns)
Performance:        5.396        4.448

GROMACS reminds you: "Sincerity is the key to success. Once you can fake that you’ve got it made." (Groucho Marx)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     160514.130971      task-clock (msec)         #    1.000 CPUs utilized          
             2,506      context-switches          #    0.016 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
            12,500      page-faults               #    0.078 K/sec                  
   448,369,851,813      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,426,406,532      instructions              #    2.36  insns per cycle        
    16,061,400,840      branches                  #  100.062 M/sec                  
       288,582,610      branch-misses             #    1.80% of all branches        

     160.504237375 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.22#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.22#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.22#

               Core t (s)   Wall t (s)        (%)
       Time:      160.079      160.079      100.0
                 (ns/day)    (hour/ns)
Performance:        5.398        4.446

GROMACS reminds you: "It Was My Pleasure" (Pulp Fiction)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     160449.600728      task-clock (msec)         #    1.000 CPUs utilized          
             2,522      context-switches          #    0.016 K/sec                  
                 4      cpu-migrations            #    0.000 K/sec                  
            12,498      page-faults               #    0.078 K/sec                  
   448,178,758,694      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,477,379,459      instructions              #    2.36  insns per cycle        
    16,062,085,582      branches                  #  100.107 M/sec                  
       289,262,473      branch-misses             #    1.80% of all branches        

     160.439112656 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.23#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.23#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.23#

               Core t (s)   Wall t (s)        (%)
       Time:      158.792      158.792      100.0
                 (ns/day)    (hour/ns)
Performance:        5.442        4.410

GROMACS reminds you: "Programming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to build bigger and better idiots. So far, the universe is winning." (Rick Cook)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159158.558048      task-clock (msec)         #    1.000 CPUs utilized          
             2,533      context-switches          #    0.016 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
            12,500      page-faults               #    0.079 K/sec                  
   444,577,152,800      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,573,694,246      instructions              #    2.38  insns per cycle        
    16,058,007,276      branches                  #  100.893 M/sec                  
       288,522,352      branch-misses             #    1.80% of all branches        

     159.150748350 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.24#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.24#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.24#

               Core t (s)   Wall t (s)        (%)
       Time:      158.768      158.768      100.0
                 (ns/day)    (hour/ns)
Performance:        5.443        4.409

GROMACS reminds you: "As Always Your Logic Is Impeccable" (Tuvok)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159131.413194      task-clock (msec)         #    1.000 CPUs utilized          
             2,663      context-switches          #    0.017 K/sec                  
                 7      cpu-migrations            #    0.000 K/sec                  
            12,498      page-faults               #    0.079 K/sec                  
   444,494,786,541      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,388,440,606      instructions              #    2.38  insns per cycle        
    16,074,913,919      branches                  #  101.017 M/sec                  
       288,078,356      branch-misses             #    1.79% of all branches        

     159.124264018 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.25#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.25#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.25#

               Core t (s)   Wall t (s)        (%)
       Time:      159.208      159.208      100.0
                 (ns/day)    (hour/ns)
Performance:        5.428        4.422

GROMACS reminds you: "Kick the Dog and You Will Die" (Magnapop)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159576.032032      task-clock (msec)         #    1.000 CPUs utilized          
             2,536      context-switches          #    0.016 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
            12,496      page-faults               #    0.078 K/sec                  
   445,765,624,213      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,440,311,208      instructions              #    2.37  insns per cycle        
    16,082,305,352      branches                  #  100.781 M/sec                  
       289,629,667      branch-misses             #    1.80% of all branches        

     159.569760238 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.26#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.26#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.26#

               Core t (s)   Wall t (s)        (%)
       Time:      158.879      158.879      100.0
                 (ns/day)    (hour/ns)
Performance:        5.439        4.412

GROMACS reminds you: "I Solve Problems" (Pulp Fiction)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159245.064756      task-clock (msec)         #    1.000 CPUs utilized          
             2,544      context-switches          #    0.016 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
            12,497      page-faults               #    0.078 K/sec                  
   444,821,345,501      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,474,687,173      instructions              #    2.38  insns per cycle        
    16,074,920,922      branches                  #  100.945 M/sec                  
       289,225,296      branch-misses             #    1.80% of all branches        

     159.234639007 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.27#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.27#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.27#

               Core t (s)   Wall t (s)        (%)
       Time:      158.619      158.619      100.0
                 (ns/day)    (hour/ns)
Performance:        5.448        4.405

GROMACS reminds you: "I admired Bohr very much. We had long talks together, long talks in which Bohr did practically all the talking." (Paul Dirac)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     158980.677013      task-clock (msec)         #    1.000 CPUs utilized          
             2,670      context-switches          #    0.017 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            12,502      page-faults               #    0.079 K/sec                  
   444,074,770,281      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,315,329,990      instructions              #    2.38  insns per cycle        
    16,059,930,235      branches                  #  101.018 M/sec                  
       289,293,314      branch-misses             #    1.80% of all branches        

     158.977741138 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.28#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.28#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.28#

               Core t (s)   Wall t (s)        (%)
       Time:      158.498      158.498      100.0
                 (ns/day)    (hour/ns)
Performance:        5.452        4.402

GROMACS reminds you: "All Beauty Must Die" (Nick Cave)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     158867.095869      task-clock (msec)         #    1.000 CPUs utilized          
             2,536      context-switches          #    0.016 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
            12,497      page-faults               #    0.079 K/sec                  
   443,752,151,197      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,242,679,005      instructions              #    2.38  insns per cycle        
    16,059,282,562      branches                  #  101.086 M/sec                  
       288,913,906      branch-misses             #    1.80% of all branches        

     158.855200067 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.3
Executable:   /opt/gromacs/bin/gmx_mpi
Data prefix:  /opt/gromacs
Working dir:  /home/mingtha/gromacs/v2016.3/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.29#

NOTE: Error occurred during GPU detection:
      CUDA driver version is insufficient for CUDA runtime version
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 1 node with total 24 cores, 24 logical cores, 0 compatible GPUs
Hardware detected on host comet-13-49.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016.3 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.29#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.29#

               Core t (s)   Wall t (s)        (%)
       Time:      159.074      159.074      100.0
                 (ns/day)    (hour/ns)
Performance:        5.433        4.418

GROMACS reminds you: "Microsecond Here I Come" (P.J. Van Maaren)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     159437.167243      task-clock (msec)         #    1.000 CPUs utilized          
             2,549      context-switches          #    0.016 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
            12,499      page-faults               #    0.078 K/sec                  
   445,365,708,588      cycles                    #    2.793 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 1,056,500,705,338      instructions              #    2.37  insns per cycle        
    16,067,478,801      branches                  #  100.776 M/sec                  
       288,455,497      branch-misses             #    1.80% of all branches        

     159.433903862 seconds time elapsed

