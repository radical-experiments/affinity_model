                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.15#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.15#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.15#

               Core t (s)   Wall t (s)        (%)
       Time:      182.672      182.672      100.0
                 (ns/day)    (hour/ns)
Performance:        4.731        5.073

GROMACS reminds you: "I Calculate My Birthright" (P.J. Harvey)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     181501.239761      task-clock (msec)         #    0.989 CPUs utilized          
            20,060      context-switches          #    0.111 K/sec                  
             1,086      cpu-migrations            #    0.006 K/sec                  
           122,560      page-faults               #    0.675 K/sec                  
   456,585,857,794      cycles                    #    2.516 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   869,037,770,192      instructions              #    1.90  insns per cycle        
    15,912,194,667      branches                  #   87.670 M/sec                  
       344,620,604      branch-misses             #    2.17% of all branches        

     183.444313688 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.16#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.16#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.16#

               Core t (s)   Wall t (s)        (%)
       Time:      184.051      184.051      100.0
                 (ns/day)    (hour/ns)
Performance:        4.695        5.111

GROMACS reminds you: "They don't have any beavers in India, so they have to simulate them" (The Tubes)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     182797.856333      task-clock (msec)         #    0.989 CPUs utilized          
            20,386      context-switches          #    0.112 K/sec                  
             1,052      cpu-migrations            #    0.006 K/sec                  
            65,584      page-faults               #    0.359 K/sec                  
   456,271,950,039      cycles                    #    2.496 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   868,058,622,352      instructions              #    1.90  insns per cycle        
    15,809,346,250      branches                  #   86.485 M/sec                  
       330,586,944      branch-misses             #    2.09% of all branches        

     184.787929804 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.17#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.17#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.17#

               Core t (s)   Wall t (s)        (%)
       Time:      183.705      183.705      100.0
                 (ns/day)    (hour/ns)
Performance:        4.704        5.102

GROMACS reminds you: "Wild Pointers Couldn't Drag Me Away" (K.A. Feenstra)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     182475.485944      task-clock (msec)         #    0.989 CPUs utilized          
            20,253      context-switches          #    0.111 K/sec                  
             1,075      cpu-migrations            #    0.006 K/sec                  
            93,147      page-faults               #    0.510 K/sec                  
   456,917,001,391      cycles                    #    2.504 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   869,321,974,365      instructions              #    1.90  insns per cycle        
    16,008,941,363      branches                  #   87.732 M/sec                  
       357,329,269      branch-misses             #    2.23% of all branches        

     184.437997626 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.18#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.18#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.18#

               Core t (s)   Wall t (s)        (%)
       Time:      182.746      182.746      100.0
                 (ns/day)    (hour/ns)
Performance:        4.729        5.075

GROMACS reminds you: "If at one time or another I have brushed a few colleagues the wrong way, I must apologize: I had not realized that they were covered with fur." (Edwin Chargaff)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     181537.338380      task-clock (msec)         #    0.989 CPUs utilized          
            20,197      context-switches          #    0.111 K/sec                  
             1,090      cpu-migrations            #    0.006 K/sec                  
            57,926      page-faults               #    0.319 K/sec                  
   456,360,450,047      cycles                    #    2.514 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   867,619,999,112      instructions              #    1.90  insns per cycle        
    15,792,848,374      branches                  #   86.995 M/sec                  
       330,534,624      branch-misses             #    2.09% of all branches        

     183.477115427 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.19#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.19#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.19#

               Core t (s)   Wall t (s)        (%)
       Time:      184.545      184.545      100.0
                 (ns/day)    (hour/ns)
Performance:        4.683        5.125

GROMACS reminds you: "Jesus Built My Hotrod" (Ministry)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     183288.562331      task-clock (msec)         #    0.989 CPUs utilized          
            20,441      context-switches          #    0.112 K/sec                  
             1,092      cpu-migrations            #    0.006 K/sec                  
           101,891      page-faults               #    0.556 K/sec                  
   456,947,657,591      cycles                    #    2.493 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   868,840,807,026      instructions              #    1.90  insns per cycle        
    15,951,152,497      branches                  #   87.028 M/sec                  
       344,904,835      branch-misses             #    2.16% of all branches        

     185.266619190 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.20#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.20#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.20#

               Core t (s)   Wall t (s)        (%)
       Time:      185.255      185.255      100.0
                 (ns/day)    (hour/ns)
Performance:        4.665        5.145

GROMACS reminds you: "I love fools' experiments. I am always making them." (Charles Darwin)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     183753.401753      task-clock (msec)         #    0.988 CPUs utilized          
            20,424      context-switches          #    0.111 K/sec                  
             1,057      cpu-migrations            #    0.006 K/sec                  
           124,349      page-faults               #    0.677 K/sec                  
   457,232,270,620      cycles                    #    2.488 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   869,367,883,527      instructions              #    1.90  insns per cycle        
    15,989,281,198      branches                  #   87.015 M/sec                  
       356,314,456      branch-misses             #    2.23% of all branches        

     185.991795101 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.21#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.21#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.21#

               Core t (s)   Wall t (s)        (%)
       Time:      185.273      185.273      100.0
                 (ns/day)    (hour/ns)
Performance:        4.664        5.145

GROMACS reminds you: "The determined Real Programmer can write FORTRAN programs in any language." (Ed Post)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     184052.762321      task-clock (msec)         #    0.989 CPUs utilized          
            20,382      context-switches          #    0.111 K/sec                  
             1,075      cpu-migrations            #    0.006 K/sec                  
            54,597      page-faults               #    0.297 K/sec                  
   460,327,008,476      cycles                    #    2.501 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   881,993,719,786      instructions              #    1.92  insns per cycle        
    16,362,425,362      branches                  #   88.901 M/sec                  
       371,016,306      branch-misses             #    2.27% of all branches        

     186.051927780 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.22#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.22#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.22#

               Core t (s)   Wall t (s)        (%)
       Time:      185.707      185.707      100.0
                 (ns/day)    (hour/ns)
Performance:        4.653        5.158

GROMACS reminds you: "I'm no model lady. A model's just an imitation of the real thing." (Mae West)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     184538.359924      task-clock (msec)         #    0.989 CPUs utilized          
            20,550      context-switches          #    0.111 K/sec                  
             1,104      cpu-migrations            #    0.006 K/sec                  
            71,832      page-faults               #    0.389 K/sec                  
   460,544,055,325      cycles                    #    2.496 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   879,932,942,272      instructions              #    1.91  insns per cycle        
    16,152,618,717      branches                  #   87.530 M/sec                  
       341,335,232      branch-misses             #    2.11% of all branches        

     186.519030315 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.23#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.23#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.23#

               Core t (s)   Wall t (s)        (%)
       Time:      183.339      183.339      100.0
                 (ns/day)    (hour/ns)
Performance:        4.714        5.092

GROMACS reminds you: "This really is a pretty scene, could you ask your kid to smile please?" (Joe Jackson)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     182148.451513      task-clock (msec)         #    0.989 CPUs utilized          
            20,225      context-switches          #    0.111 K/sec                  
             1,119      cpu-migrations            #    0.006 K/sec                  
            50,134      page-faults               #    0.275 K/sec                  
   457,195,231,941      cycles                    #    2.510 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   868,131,383,483      instructions              #    1.90  insns per cycle        
    15,876,371,841      branches                  #   87.162 M/sec                  
       343,605,904      branch-misses             #    2.16% of all branches        

     184.112734913 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.24#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.24#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.24#

               Core t (s)   Wall t (s)        (%)
       Time:      185.339      185.339      100.0
                 (ns/day)    (hour/ns)
Performance:        4.663        5.147

GROMACS reminds you: "Here, kitty, kitty..." (Erwin Schroedinger)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     184109.420138      task-clock (msec)         #    0.989 CPUs utilized          
            20,446      context-switches          #    0.111 K/sec                  
             1,111      cpu-migrations            #    0.006 K/sec                  
           148,632      page-faults               #    0.807 K/sec                  
   460,615,091,764      cycles                    #    2.502 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   885,503,982,023      instructions              #    1.92  insns per cycle        
    16,627,235,951      branches                  #   90.312 M/sec                  
       365,262,099      branch-misses             #    2.20% of all branches        

     186.147416256 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.25#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.25#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.25#

               Core t (s)   Wall t (s)        (%)
       Time:      184.780      184.780      100.0
                 (ns/day)    (hour/ns)
Performance:        4.677        5.132

GROMACS reminds you: "Have a Nice Day" (R. McDonald)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     183510.566624      task-clock (msec)         #    0.989 CPUs utilized          
            20,423      context-switches          #    0.111 K/sec                  
             1,107      cpu-migrations            #    0.006 K/sec                  
            91,063      page-faults               #    0.496 K/sec                  
   458,740,530,766      cycles                    #    2.500 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   878,108,840,069      instructions              #    1.91  insns per cycle        
    16,426,293,181      branches                  #   89.511 M/sec                  
       343,686,849      branch-misses             #    2.09% of all branches        

     185.515208529 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.26#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.26#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.26#

               Core t (s)   Wall t (s)        (%)
       Time:      184.932      184.932      100.0
                 (ns/day)    (hour/ns)
Performance:        4.673        5.136

GROMACS reminds you: "This isn't right. This isn't even wrong." (Wolfgang Pauli)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     183767.427626      task-clock (msec)         #    0.989 CPUs utilized          
            20,509      context-switches          #    0.112 K/sec                  
             1,093      cpu-migrations            #    0.006 K/sec                  
           131,880      page-faults               #    0.718 K/sec                  
   459,759,675,142      cycles                    #    2.502 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   877,934,871,361      instructions              #    1.91  insns per cycle        
    16,447,888,586      branches                  #   89.504 M/sec                  
       340,581,052      branch-misses             #    2.07% of all branches        

     185.723194498 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.27#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.27#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.27#

               Core t (s)   Wall t (s)        (%)
       Time:      183.435      183.435      100.0
                 (ns/day)    (hour/ns)
Performance:        4.711        5.094

GROMACS reminds you: "Jede der Scherben spiegelt das Licht" (Wir sind Helden)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     182200.856417      task-clock (msec)         #    0.989 CPUs utilized          
            20,255      context-switches          #    0.111 K/sec                  
             1,067      cpu-migrations            #    0.006 K/sec                  
            63,138      page-faults               #    0.347 K/sec                  
   456,165,196,381      cycles                    #    2.504 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   868,311,302,932      instructions              #    1.90  insns per cycle        
    15,904,551,847      branches                  #   87.291 M/sec                  
       343,713,738      branch-misses             #    2.16% of all branches        

     184.165385718 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.28#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.28#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.28#

               Core t (s)   Wall t (s)        (%)
       Time:      184.284      184.284      100.0
                 (ns/day)    (hour/ns)
Performance:        4.689        5.118

GROMACS reminds you: "It Was My Pleasure" (Pulp Fiction)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     183120.854201      task-clock (msec)         #    0.988 CPUs utilized          
            20,341      context-switches          #    0.111 K/sec                  
             1,088      cpu-migrations            #    0.006 K/sec                  
            88,450      page-faults               #    0.483 K/sec                  
   459,397,339,866      cycles                    #    2.509 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   880,885,498,814      instructions              #    1.92  insns per cycle        
    16,257,631,659      branches                  #   88.781 M/sec                  
       355,154,298      branch-misses             #    2.18% of all branches        

     185.281930551 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_1
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.29#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r255.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.29#
starting mdrun 'Protein in water'
5000 steps,     10.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.29#

               Core t (s)   Wall t (s)        (%)
       Time:      185.559      185.559      100.0
                 (ns/day)    (hour/ns)
Performance:        4.657        5.153

GROMACS reminds you: "Microsecond Here I Come" (P.J. Van Maaren)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     184334.656186      task-clock (msec)         #    0.989 CPUs utilized          
            20,526      context-switches          #    0.111 K/sec                  
             1,116      cpu-migrations            #    0.006 K/sec                  
           101,682      page-faults               #    0.552 K/sec                  
   458,377,341,769      cycles                    #    2.487 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
   868,426,192,107      instructions              #    1.89  insns per cycle        
    15,924,961,135      branches                  #   86.392 M/sec                  
       343,435,290      branch-misses             #    2.16% of all branches        

     186.306648981 seconds time elapsed

