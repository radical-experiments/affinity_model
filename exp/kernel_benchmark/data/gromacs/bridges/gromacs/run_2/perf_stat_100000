                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.13#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.13#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.14#

               Core t (s)   Wall t (s)        (%)
       Time:     3904.225     3904.225      100.0
                         1h05:04
                 (ns/day)    (hour/ns)
Performance:        4.426        5.422

GROMACS reminds you: "I Had So Many Problem, and Then I Got Me a Walkman" (F. Black)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3866130.089342      task-clock (msec)         #    0.990 CPUs utilized          
           400,479      context-switches          #    0.104 K/sec                  
            21,162      cpu-migrations            #    0.005 K/sec                  
           279,514      page-faults               #    0.072 K/sec                  
 9,214,552,813,390      cycles                    #    2.383 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,323,631,933,961      instructions              #    1.88  insns per cycle        
   309,876,498,625      branches                  #   80.152 M/sec                  
     6,612,677,483      branch-misses             #    2.13% of all branches        

    3904.935029360 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.14#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.14#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.15#

               Core t (s)   Wall t (s)        (%)
       Time:     3890.205     3890.205      100.0
                         1h04:50
                 (ns/day)    (hour/ns)
Performance:        4.442        5.403

GROMACS reminds you: "No One Could Foresee the End That Came So Fast" (Slayer)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3852975.280931      task-clock (msec)         #    0.990 CPUs utilized          
           397,919      context-switches          #    0.103 K/sec                  
            20,724      cpu-migrations            #    0.005 K/sec                  
           220,370      page-faults               #    0.057 K/sec                  
 9,228,554,498,929      cycles                    #    2.395 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,569,271,068,878      instructions              #    1.90  insns per cycle        
   316,948,377,767      branches                  #   82.261 M/sec                  
     6,824,416,958      branch-misses             #    2.15% of all branches        

    3891.100696130 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.15#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.15#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.16#

               Core t (s)   Wall t (s)        (%)
       Time:     3910.624     3910.624      100.0
                         1h05:10
                 (ns/day)    (hour/ns)
Performance:        4.419        5.431

GROMACS reminds you: "FORTRAN was the language of choice for the same reason that three-legged races are popular." (Ken Thompson)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3872828.292692      task-clock (msec)         #    0.990 CPUs utilized          
           401,298      context-switches          #    0.104 K/sec                  
            21,399      cpu-migrations            #    0.006 K/sec                  
           351,772      page-faults               #    0.091 K/sec                  
 9,225,736,927,050      cycles                    #    2.382 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,337,174,968,384      instructions              #    1.88  insns per cycle        
   311,614,998,998      branches                  #   80.462 M/sec                  
     6,849,979,260      branch-misses             #    2.20% of all branches        

    3911.446390192 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.16#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.16#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.17#

               Core t (s)   Wall t (s)        (%)
       Time:     3900.178     3900.178      100.0
                         1h05:00
                 (ns/day)    (hour/ns)
Performance:        4.431        5.417

GROMACS reminds you: "If you want to destroy my sweater, hold this thread as I walk away." (Weezer)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3863123.715776      task-clock (msec)         #    0.990 CPUs utilized          
           400,055      context-switches          #    0.104 K/sec                  
            20,815      cpu-migrations            #    0.005 K/sec                  
           182,053      page-faults               #    0.047 K/sec                  
 9,219,137,133,864      cycles                    #    2.386 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,348,552,571,122      instructions              #    1.88  insns per cycle        
   313,169,820,093      branches                  #   81.066 M/sec                  
     7,098,742,714      branch-misses             #    2.27% of all branches        

    3900.953606562 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.17#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.17#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.18#

               Core t (s)   Wall t (s)        (%)
       Time:     3940.550     3940.550      100.0
                         1h05:40
                 (ns/day)    (hour/ns)
Performance:        4.385        5.473

GROMACS reminds you: "If There Is No Guitar In The House, You Know It's Owner Can Not Be Trusted" (Gogol Bordello)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3903504.783932      task-clock (msec)         #    0.990 CPUs utilized          
           404,100      context-switches          #    0.104 K/sec                  
            21,054      cpu-migrations            #    0.005 K/sec                  
           303,981      page-faults               #    0.078 K/sec                  
 9,319,498,087,559      cycles                    #    2.387 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,630,360,064,861      instructions              #    1.89  insns per cycle        
   317,681,948,203      branches                  #   81.384 M/sec                  
     6,805,506,264      branch-misses             #    2.14% of all branches        

    3941.335266431 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.18#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.18#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.19#

               Core t (s)   Wall t (s)        (%)
       Time:     3901.567     3901.567      100.0
                         1h05:01
                 (ns/day)    (hour/ns)
Performance:        4.429        5.419

GROMACS reminds you: "On average, it takes twenty years for the world's largest super computer to shrink down to the size of your laptop." (Pete Beckman)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3864492.633794      task-clock (msec)         #    0.990 CPUs utilized          
           400,717      context-switches          #    0.104 K/sec                  
            20,969      cpu-migrations            #    0.005 K/sec                  
           290,089      page-faults               #    0.075 K/sec                  
 9,238,874,473,394      cycles                    #    2.391 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,349,532,749,299      instructions              #    1.88  insns per cycle        
   313,255,963,055      branches                  #   81.060 M/sec                  
     7,117,335,732      branch-misses             #    2.27% of all branches        

    3902.401770478 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.19#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.19#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.20#

               Core t (s)   Wall t (s)        (%)
       Time:     3962.184     3962.184      100.0
                         1h06:02
                 (ns/day)    (hour/ns)
Performance:        4.361        5.503

GROMACS reminds you: "Everything Must Go" (Red Hot Chili Peppers)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3923766.756184      task-clock (msec)         #    0.990 CPUs utilized          
           406,981      context-switches          #    0.104 K/sec                  
            21,981      cpu-migrations            #    0.006 K/sec                  
           270,231      page-faults               #    0.069 K/sec                  
 9,339,952,065,770      cycles                    #    2.380 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,440,382,067,872      instructions              #    1.87  insns per cycle        
   320,458,086,871      branches                  #   81.671 M/sec                  
     7,316,255,728      branch-misses             #    2.28% of all branches        

    3962.969731455 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.20#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.20#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.21#

               Core t (s)   Wall t (s)        (%)
       Time:     3898.776     3898.776      100.0
                         1h04:58
                 (ns/day)    (hour/ns)
Performance:        4.432        5.415

GROMACS reminds you: "Don't You Wish You Never Met Her, Dirty Blue Gene?" (Captain Beefheart)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3861289.080309      task-clock (msec)         #    0.990 CPUs utilized          
           400,089      context-switches          #    0.104 K/sec                  
            21,419      cpu-migrations            #    0.006 K/sec                  
           278,685      page-faults               #    0.072 K/sec                  
 9,221,728,724,152      cycles                    #    2.388 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,323,588,702,527      instructions              #    1.88  insns per cycle        
   309,949,759,807      branches                  #   80.271 M/sec                  
     6,588,008,177      branch-misses             #    2.13% of all branches        

    3899.510261121 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.21#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.21#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.22#

               Core t (s)   Wall t (s)        (%)
       Time:     3888.491     3888.491      100.0
                         1h04:48
                 (ns/day)    (hour/ns)
Performance:        4.444        5.401

GROMACS reminds you: "If you want to destroy my sweater, hold this thread as I walk away." (Weezer)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3851637.980209      task-clock (msec)         #    0.990 CPUs utilized          
           398,568      context-switches          #    0.103 K/sec                  
            20,870      cpu-migrations            #    0.005 K/sec                  
           325,000      page-faults               #    0.084 K/sec                  
 9,214,452,255,486      cycles                    #    2.392 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,324,993,656,687      instructions              #    1.88  insns per cycle        
   310,239,434,728      branches                  #   80.547 M/sec                  
     6,582,498,602      branch-misses             #    2.12% of all branches        

    3889.265754495 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.22#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.22#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.23#

               Core t (s)   Wall t (s)        (%)
       Time:     3901.013     3901.013      100.0
                         1h05:01
                 (ns/day)    (hour/ns)
Performance:        4.430        5.418

GROMACS reminds you: "All I Ever Wanted Was Your Life" (Red Hot Chili Peppers)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3864363.654446      task-clock (msec)         #    0.990 CPUs utilized          
           399,868      context-switches          #    0.103 K/sec                  
            21,338      cpu-migrations            #    0.006 K/sec                  
           294,396      page-faults               #    0.076 K/sec                  
 9,242,346,547,613      cycles                    #    2.392 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,575,642,710,351      instructions              #    1.90  insns per cycle        
   318,994,630,563      branches                  #   82.548 M/sec                  
     7,069,704,167      branch-misses             #    2.22% of all branches        

    3901.835552244 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.23#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.23#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.24#

               Core t (s)   Wall t (s)        (%)
       Time:     3919.756     3919.756      100.0
                         1h05:19
                 (ns/day)    (hour/ns)
Performance:        4.408        5.444

GROMACS reminds you: "Hey Man You Know, I'm Really OK" (Offspring)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3882308.211307      task-clock (msec)         #    0.990 CPUs utilized          
           402,199      context-switches          #    0.104 K/sec                  
            21,926      cpu-migrations            #    0.006 K/sec                  
           311,656      page-faults               #    0.080 K/sec                  
 9,248,128,910,197      cycles                    #    2.382 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,527,866,725,461      instructions              #    1.90  insns per cycle        
   322,756,069,570      branches                  #   83.135 M/sec                  
     6,806,687,961      branch-misses             #    2.11% of all branches        

    3920.579599402 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.24#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.24#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.25#

               Core t (s)   Wall t (s)        (%)
       Time:     3939.837     3939.837      100.0
                         1h05:39
                 (ns/day)    (hour/ns)
Performance:        4.386        5.472

GROMACS reminds you: "I spent a lot of money on booze, birds and fast cars. The rest I just squandered." (George Best)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3902170.678488      task-clock (msec)         #    0.990 CPUs utilized          
           405,093      context-switches          #    0.104 K/sec                  
            20,846      cpu-migrations            #    0.005 K/sec                  
           221,649      page-faults               #    0.057 K/sec                  
 9,315,242,186,066      cycles                    #    2.387 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,712,060,854,171      instructions              #    1.90  insns per cycle        
   330,787,221,141      branches                  #   84.770 M/sec                  
     7,098,678,404      branch-misses             #    2.15% of all branches        

    3940.627445882 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.25#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.25#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.26#

               Core t (s)   Wall t (s)        (%)
       Time:     3938.167     3938.167      100.0
                         1h05:38
                 (ns/day)    (hour/ns)
Performance:        4.388        5.470

GROMACS reminds you: "Ease Myself Into the Body Bag" (P.J. Harvey)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3900577.483384      task-clock (msec)         #    0.990 CPUs utilized          
           403,611      context-switches          #    0.103 K/sec                  
            20,838      cpu-migrations            #    0.005 K/sec                  
           244,559      page-faults               #    0.063 K/sec                  
 9,307,845,434,299      cycles                    #    2.386 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,689,321,726,673      instructions              #    1.90  insns per cycle        
   329,106,174,880      branches                  #   84.374 M/sec                  
     6,824,126,306      branch-misses             #    2.07% of all branches        

    3938.975583354 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.26#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.26#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.27#

               Core t (s)   Wall t (s)        (%)
       Time:     3932.135     3932.135      100.0
                         1h05:32
                 (ns/day)    (hour/ns)
Performance:        4.395        5.461

GROMACS reminds you: "In the End Science Comes Down to Praying" (P. v.d. Berg)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3894436.366716      task-clock (msec)         #    0.990 CPUs utilized          
           403,862      context-switches          #    0.104 K/sec                  
            21,103      cpu-migrations            #    0.005 K/sec                  
           284,995      page-faults               #    0.073 K/sec                  
 9,268,293,303,086      cycles                    #    2.380 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,334,931,454,414      instructions              #    1.87  insns per cycle        
   311,945,973,231      branches                  #   80.100 M/sec                  
     6,839,716,723      branch-misses             #    2.19% of all branches        

    3932.935257080 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_2
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.27#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r247.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.27#
starting mdrun 'Protein in water'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.28#

               Core t (s)   Wall t (s)        (%)
       Time:     3938.613     3938.613      100.0
                         1h05:38
                 (ns/day)    (hour/ns)
Performance:        4.387        5.470

GROMACS reminds you: "Therefore, things must be learned only to be unlearned again or, more likely, to be corrected." (Richard Feynman)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    3900928.685650      task-clock (msec)         #    0.990 CPUs utilized          
           405,326      context-switches          #    0.104 K/sec                  
            20,773      cpu-migrations            #    0.005 K/sec                  
           283,405      page-faults               #    0.073 K/sec                  
 9,315,777,186,670      cycles                    #    2.388 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
17,813,188,241,908      instructions              #    1.91  insns per cycle        
   324,505,546,965      branches                  #   83.187 M/sec                  
     7,040,200,115      branch-misses             #    2.17% of all branches        

    3939.424941638 seconds time elapsed

