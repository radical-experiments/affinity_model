                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.60#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.60#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.60#

               Core t (s)   Wall t (s)        (%)
       Time:     1951.007     1951.007      100.0
                         32:31
                 (ns/day)    (hour/ns)
Performance:        4.429        5.419

GROMACS reminds you: "During my undergraduate work I concluded that electrostatics is unlikely to be important [for enzymes]" (Arieh Warshel, Nobel lecture 2013)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1932497.704319      task-clock (msec)         #    0.990 CPUs utilized          
           201,559      context-switches          #    0.104 K/sec                  
             9,624      cpu-migrations            #    0.005 K/sec                  
           144,250      page-faults               #    0.075 K/sec                  
 4,595,196,401,571      cycles                    #    2.378 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,663,727,050,389      instructions              #    1.89  insns per cycle        
   154,951,068,508      branches                  #   80.182 M/sec                  
     3,302,712,022      branch-misses             #    2.13% of all branches        

    1951.802104386 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.61#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.61#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.61#

               Core t (s)   Wall t (s)        (%)
       Time:     1946.658     1946.658      100.0
                         32:26
                 (ns/day)    (hour/ns)
Performance:        4.438        5.407

GROMACS reminds you: "C has the power of assembly language and the convenience of... assembly language." (Dennis Ritchie)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1928674.273339      task-clock (msec)         #    0.990 CPUs utilized          
           200,489      context-switches          #    0.104 K/sec                  
             9,848      cpu-migrations            #    0.005 K/sec                  
           111,670      page-faults               #    0.058 K/sec                  
 4,601,598,660,681      cycles                    #    2.386 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,675,546,356,872      instructions              #    1.89  insns per cycle        
   156,595,948,648      branches                  #   81.194 M/sec                  
     3,550,106,369      branch-misses             #    2.27% of all branches        

    1947.466597063 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.62#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.62#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.62#

               Core t (s)   Wall t (s)        (%)
       Time:     1961.773     1961.773      100.0
                         32:41
                 (ns/day)    (hour/ns)
Performance:        4.404        5.449

GROMACS reminds you: "Ich Bin Ein Berliner" (J.F. Kennedy)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1943379.048423      task-clock (msec)         #    0.990 CPUs utilized          
           202,111      context-switches          #    0.104 K/sec                  
             9,836      cpu-migrations            #    0.005 K/sec                  
           163,913      page-faults               #    0.084 K/sec                  
 4,624,552,531,975      cycles                    #    2.380 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,791,510,338,696      instructions              #    1.90  insns per cycle        
   159,432,820,727      branches                  #   82.039 M/sec                  
     3,520,246,296      branch-misses             #    2.21% of all branches        

    1962.558557657 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.63#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.63#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.63#

               Core t (s)   Wall t (s)        (%)
       Time:     1983.577     1983.577      100.0
                         33:03
                 (ns/day)    (hour/ns)
Performance:        4.356        5.510

GROMACS reminds you: "I Quit My Job Blowing Leaves" (Beck)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1965030.344449      task-clock (msec)         #    0.990 CPUs utilized          
           204,324      context-switches          #    0.104 K/sec                  
             9,785      cpu-migrations            #    0.005 K/sec                  
           124,907      page-faults               #    0.064 K/sec                  
 4,686,775,570,196      cycles                    #    2.385 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,672,088,912,582      instructions              #    1.85  insns per cycle        
   156,312,595,917      branches                  #   79.547 M/sec                  
     3,543,727,844      branch-misses             #    2.27% of all branches        

    1984.394686531 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.64#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.64#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.64#

               Core t (s)   Wall t (s)        (%)
       Time:     1952.969     1952.969      100.0
                         32:32
                 (ns/day)    (hour/ns)
Performance:        4.424        5.425

GROMACS reminds you: "Here's Another Useful Quote" (S. Boot)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1934791.016128      task-clock (msec)         #    0.990 CPUs utilized          
           201,367      context-switches          #    0.104 K/sec                  
             9,932      cpu-migrations            #    0.005 K/sec                  
           158,333      page-faults               #    0.082 K/sec                  
 4,615,323,909,689      cycles                    #    2.385 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,669,792,225,704      instructions              #    1.88  insns per cycle        
   155,801,882,626      branches                  #   80.526 M/sec                  
     3,432,149,956      branch-misses             #    2.20% of all branches        

    1953.734927039 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.65#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.65#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.65#

               Core t (s)   Wall t (s)        (%)
       Time:     1948.725     1948.725      100.0
                         32:28
                 (ns/day)    (hour/ns)
Performance:        4.434        5.413

GROMACS reminds you: "I believe the big bang of self-driving cars is about to come." (Jen-Hsun Huang, CEO NVIDIA)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1930725.182922      task-clock (msec)         #    0.990 CPUs utilized          
           200,548      context-switches          #    0.104 K/sec                  
             9,608      cpu-migrations            #    0.005 K/sec                  
           157,913      page-faults               #    0.082 K/sec                  
 4,607,838,258,398      cycles                    #    2.387 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,785,246,788,953      instructions              #    1.91  insns per cycle        
   157,707,477,510      branches                  #   81.683 M/sec                  
     3,274,235,599      branch-misses             #    2.08% of all branches        

    1949.508137968 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.66#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.66#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.66#

               Core t (s)   Wall t (s)        (%)
       Time:     1941.086     1941.086      100.0
                         32:21
                 (ns/day)    (hour/ns)
Performance:        4.451        5.392

GROMACS reminds you: "No One Could Foresee the End That Came So Fast" (Slayer)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1923317.110360      task-clock (msec)         #    0.990 CPUs utilized          
           199,852      context-switches          #    0.104 K/sec                  
             9,215      cpu-migrations            #    0.005 K/sec                  
           150,608      page-faults               #    0.078 K/sec                  
 4,595,302,720,133      cycles                    #    2.389 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,668,501,642,868      instructions              #    1.89  insns per cycle        
   155,792,083,313      branches                  #   81.002 M/sec                  
     3,424,973,296      branch-misses             #    2.20% of all branches        

    1941.964294587 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.67#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.67#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.67#

               Core t (s)   Wall t (s)        (%)
       Time:     1925.412     1925.412      100.0
                         32:05
                 (ns/day)    (hour/ns)
Performance:        4.487        5.348

GROMACS reminds you: "Does All This Money Really Have To Go To Charity ?" (Rick)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1907905.806145      task-clock (msec)         #    0.991 CPUs utilized          
           198,143      context-switches          #    0.104 K/sec                  
             8,863      cpu-migrations            #    0.005 K/sec                  
           119,466      page-faults               #    0.063 K/sec                  
 4,582,589,688,655      cycles                    #    2.402 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,662,012,492,678      instructions              #    1.89  insns per cycle        
   154,894,629,674      branches                  #   81.186 M/sec                  
     3,292,653,680      branch-misses             #    2.13% of all branches        

    1926.196340368 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.68#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.68#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.68#

               Core t (s)   Wall t (s)        (%)
       Time:     1947.522     1947.522      100.0
                         32:27
                 (ns/day)    (hour/ns)
Performance:        4.436        5.410

GROMACS reminds you: "I originally implemented PME to prove that you didn't need it..." (Erik Lindahl)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1929648.907162      task-clock (msec)         #    0.990 CPUs utilized          
           200,451      context-switches          #    0.104 K/sec                  
             8,807      cpu-migrations            #    0.005 K/sec                  
           151,749      page-faults               #    0.079 K/sec                  
 4,615,600,690,214      cycles                    #    2.392 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,720,157,144,068      instructions              #    1.89  insns per cycle        
   160,201,811,763      branches                  #   83.021 M/sec                  
     3,635,648,086      branch-misses             #    2.27% of all branches        

    1948.344919923 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.69#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.69#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.69#

               Core t (s)   Wall t (s)        (%)
       Time:     1958.237     1958.237      100.0
                         32:38
                 (ns/day)    (hour/ns)
Performance:        4.412        5.439

GROMACS reminds you: "Good Music Saves your Soul" (Lemmy)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1940272.098894      task-clock (msec)         #    0.990 CPUs utilized          
           202,061      context-switches          #    0.104 K/sec                  
             8,684      cpu-migrations            #    0.004 K/sec                  
           148,086      page-faults               #    0.076 K/sec                  
 4,629,557,945,348      cycles                    #    2.386 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,809,231,688,410      instructions              #    1.90  insns per cycle        
   159,389,252,087      branches                  #   82.148 M/sec                  
     3,524,809,479      branch-misses             #    2.21% of all branches        

    1959.104546655 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.70#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.70#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.70#

               Core t (s)   Wall t (s)        (%)
       Time:     1940.916     1940.916      100.0
                         32:20
                 (ns/day)    (hour/ns)
Performance:        4.452        5.391

GROMACS reminds you: "One Cross Each" (Monty Python)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1923263.486072      task-clock (msec)         #    0.991 CPUs utilized          
           199,980      context-switches          #    0.104 K/sec                  
             8,621      cpu-migrations            #    0.004 K/sec                  
           139,438      page-faults               #    0.073 K/sec                  
 4,611,724,779,915      cycles                    #    2.398 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,790,508,709,815      instructions              #    1.91  insns per cycle        
   159,486,022,240      branches                  #   82.925 M/sec                  
     3,532,165,145      branch-misses             #    2.21% of all branches        

    1941.708638663 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.71#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.71#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.71#

               Core t (s)   Wall t (s)        (%)
       Time:     1942.991     1942.991      100.0
                         32:22
                 (ns/day)    (hour/ns)
Performance:        4.447        5.397

GROMACS reminds you: "Unfortunately, "simulation" has become increasingly misused to mean nothing more than "calculation"" (Bill Jorgensen)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1925533.266880      task-clock (msec)         #    0.991 CPUs utilized          
           199,963      context-switches          #    0.104 K/sec                  
             8,707      cpu-migrations            #    0.005 K/sec                  
           107,070      page-faults               #    0.056 K/sec                  
 4,629,042,077,901      cycles                    #    2.404 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,880,110,588,753      instructions              #    1.92  insns per cycle        
   164,070,984,342      branches                  #   85.208 M/sec                  
     3,361,461,436      branch-misses             #    2.05% of all branches        

    1943.826938104 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.72#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.72#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.72#

               Core t (s)   Wall t (s)        (%)
       Time:     1946.928     1946.928      100.0
                         32:26
                 (ns/day)    (hour/ns)
Performance:        4.438        5.408

GROMACS reminds you: "Don't Push Me, Cause I'm Close to the Edge" (Grandmaster Flash)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1929144.429367      task-clock (msec)         #    0.990 CPUs utilized          
           200,621      context-switches          #    0.104 K/sec                  
             9,561      cpu-migrations            #    0.005 K/sec                  
           139,127      page-faults               #    0.072 K/sec                  
 4,602,404,562,423      cycles                    #    2.386 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,675,135,210,924      instructions              #    1.88  insns per cycle        
   156,820,417,699      branches                  #   81.290 M/sec                  
     3,541,925,340      branch-misses             #    2.26% of all branches        

    1947.744688914 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.73#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.73#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.73#

               Core t (s)   Wall t (s)        (%)
       Time:     1961.661     1961.661      100.0
                         32:41
                 (ns/day)    (hour/ns)
Performance:        4.405        5.449

GROMACS reminds you: "This May Come As a Shock" (F. Black)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1943419.556280      task-clock (msec)         #    0.990 CPUs utilized          
           202,450      context-switches          #    0.104 K/sec                  
            10,401      cpu-migrations            #    0.005 K/sec                  
           113,535      page-faults               #    0.058 K/sec                  
 4,618,083,122,141      cycles                    #    2.376 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,720,045,138,295      instructions              #    1.89  insns per cycle        
   160,259,332,199      branches                  #   82.463 M/sec                  
     3,650,342,722      branch-misses             #    2.28% of all branches        

    1962.522711247 seconds time elapsed

                       :-) GROMACS - gmx mdrun, 2016 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016
Executable:   /opt/packages/gromacs-2016-CPU/bin/gmx_mpi
Data prefix:  /opt/packages/gromacs-2016-CPU
Working dir:  /home/mha/gromacs/run_4
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.74#

Running on 1 node with total 28 cores, 28 logical cores
Hardware detected on host r412.pvt.bridges.psc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file out.tpr, VERSION 2016 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


NOTE: Thread affinity setting failed. This can cause performance degradation.
      If you think your settings are correct, ask on the gmx-users list.


Back Off! I just backed up ener.edr to ./#ener.edr.74#
starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.74#

               Core t (s)   Wall t (s)        (%)
       Time:     1943.419     1943.419      100.0
                         32:23
                 (ns/day)    (hour/ns)
Performance:        4.446        5.398

GROMACS reminds you: "Money won't buy happiness, but it will pay the salaries of a large research staff to study the problem." (Bill Vaughan)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

    1925378.625665      task-clock (msec)         #    0.990 CPUs utilized          
           200,175      context-switches          #    0.104 K/sec                  
             9,933      cpu-migrations            #    0.005 K/sec                  
           164,641      page-faults               #    0.086 K/sec                  
 4,600,510,758,334      cycles                    #    2.389 GHz                    
   <not supported>      stalled-cycles-frontend  
   <not supported>      stalled-cycles-backend   
 8,660,982,880,461      instructions              #    1.88  insns per cycle        
   154,986,220,727      branches                  #   80.496 M/sec                  
     3,291,465,082      branch-misses             #    2.12% of all branches        

    1944.196451399 seconds time elapsed

