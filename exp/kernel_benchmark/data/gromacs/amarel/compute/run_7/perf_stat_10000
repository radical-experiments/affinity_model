                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.30#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.30#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.30#

               Core t (s)   Wall t (s)        (%)
       Time:      344.662      344.662      100.0
                 (ns/day)    (hour/ns)
Performance:        5.014        4.786

GROMACS reminds you: "There is no reason for any individual to have a computer in his home." (Ken Olsen, head of Digital Equipment Corp.)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     344686.228642      task-clock (msec)         #    1.000 CPUs utilized          
            35,522      context-switches          #    0.103 K/sec                  
                 9      cpu-migrations            #    0.000 K/sec                  
            48,378      page-faults               #    0.140 K/sec                  
 1,133,778,202,558      cycles                    #    3.289 GHz                    
   615,076,323,388      stalled-cycles-frontend   #   54.25% frontend cycles idle   
   113,197,552,957      stalled-cycles-backend    #    9.98% backend cycles idle    
 1,830,647,946,478      instructions              #    1.61  insn per cycle         
                                                  #    0.34  stalled cycles per insn
    50,059,729,811      branches                  #  145.233 M/sec                  
       737,688,095      branch-misses             #    1.47% of all branches        

     344.807521678 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.31#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.31#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.31#

               Core t (s)   Wall t (s)        (%)
       Time:      337.167      337.167      100.0
                 (ns/day)    (hour/ns)
Performance:        5.126        4.682

GROMACS reminds you: "I was detained, I was restrained" (The Smiths)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337200.459608      task-clock (msec)         #    1.000 CPUs utilized          
            34,755      context-switches          #    0.103 K/sec                  
                 8      cpu-migrations            #    0.000 K/sec                  
            48,882      page-faults               #    0.145 K/sec                  
 1,109,160,887,979      cycles                    #    3.289 GHz                    
   590,565,063,918      stalled-cycles-frontend   #   53.24% frontend cycles idle   
   103,644,812,815      stalled-cycles-backend    #    9.34% backend cycles idle    
 1,830,897,781,069      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,055,918,491      branches                  #  148.446 M/sec                  
       737,667,190      branch-misses             #    1.47% of all branches        

     337.316172003 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.32#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.32#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.32#

               Core t (s)   Wall t (s)        (%)
       Time:      337.525      337.525      100.0
                 (ns/day)    (hour/ns)
Performance:        5.120        4.687

GROMACS reminds you: "I'm Only Faking When I Get It Right" (Soundgarden)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337558.425308      task-clock (msec)         #    1.000 CPUs utilized          
            34,797      context-switches          #    0.103 K/sec                  
                11      cpu-migrations            #    0.000 K/sec                  
            48,411      page-faults               #    0.143 K/sec                  
 1,110,343,564,942      cycles                    #    3.289 GHz                    
   591,473,773,303      stalled-cycles-frontend   #   53.27% frontend cycles idle   
   105,298,398,450      stalled-cycles-backend    #    9.48% backend cycles idle    
 1,831,193,240,953      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,061,322,185      branches                  #  148.304 M/sec                  
       741,086,453      branch-misses             #    1.48% of all branches        

     337.674809381 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.33#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.33#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.33#

               Core t (s)   Wall t (s)        (%)
       Time:      338.094      338.094      100.0
                 (ns/day)    (hour/ns)
Performance:        5.112        4.695

GROMACS reminds you: "Safety lights are for dudes" (Ghostbusters 2016)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     338115.023618      task-clock (msec)         #    1.000 CPUs utilized          
            34,850      context-switches          #    0.103 K/sec                  
                13      cpu-migrations            #    0.000 K/sec                  
            48,680      page-faults               #    0.144 K/sec                  
 1,112,226,809,584      cycles                    #    3.289 GHz                    
   593,516,761,024      stalled-cycles-frontend   #   53.36% frontend cycles idle   
   105,489,035,413      stalled-cycles-backend    #    9.48% backend cycles idle    
 1,831,328,688,905      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,062,878,736      branches                  #  148.065 M/sec                  
       736,020,557      branch-misses             #    1.47% of all branches        

     338.243044387 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.34#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.34#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.34#

               Core t (s)   Wall t (s)        (%)
       Time:      337.639      337.639      100.0
                 (ns/day)    (hour/ns)
Performance:        5.118        4.689

GROMACS reminds you: "If You're So Special Why aren't You Dead ?" (The Breeders)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337666.042820      task-clock (msec)         #    1.000 CPUs utilized          
            34,808      context-switches          #    0.103 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            48,545      page-faults               #    0.144 K/sec                  
 1,110,772,496,315      cycles                    #    3.290 GHz                    
   591,827,567,704      stalled-cycles-frontend   #   53.28% frontend cycles idle   
   104,303,172,981      stalled-cycles-backend    #    9.39% backend cycles idle    
 1,831,656,674,867      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,062,072,234      branches                  #  148.259 M/sec                  
       738,525,183      branch-misses             #    1.48% of all branches        

     337.790052111 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.35#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.35#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.35#

               Core t (s)   Wall t (s)        (%)
       Time:      336.362      336.362      100.0
                 (ns/day)    (hour/ns)
Performance:        5.138        4.671

GROMACS reminds you: "Beat On the Brat With a Baseball Bat" (The Ramones)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     336374.150585      task-clock (msec)         #    1.000 CPUs utilized          
            34,673      context-switches          #    0.103 K/sec                  
                16      cpu-migrations            #    0.000 K/sec                  
            39,579      page-faults               #    0.118 K/sec                  
 1,106,489,504,188      cycles                    #    3.289 GHz                    
   587,624,223,340      stalled-cycles-frontend   #   53.11% frontend cycles idle   
   101,691,844,598      stalled-cycles-backend    #    9.19% backend cycles idle    
 1,831,082,881,940      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,060,272,777      branches                  #  148.823 M/sec                  
       740,941,671      branch-misses             #    1.48% of all branches        

     336.511001065 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.36#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.36#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.36#

               Core t (s)   Wall t (s)        (%)
       Time:      341.177      341.177      100.0
                 (ns/day)    (hour/ns)
Performance:        5.065        4.738

GROMACS reminds you: "You always pass failure on the way to success." (Mickey Rooney)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     341195.344624      task-clock (msec)         #    1.000 CPUs utilized          
            35,202      context-switches          #    0.103 K/sec                  
                 9      cpu-migrations            #    0.000 K/sec                  
            49,105      page-faults               #    0.144 K/sec                  
 1,122,345,050,558      cycles                    #    3.289 GHz                    
   603,421,723,249      stalled-cycles-frontend   #   53.76% frontend cycles idle   
   108,767,503,172      stalled-cycles-backend    #    9.69% backend cycles idle    
 1,831,323,599,312      instructions              #    1.63  insn per cycle         
                                                  #    0.33  stalled cycles per insn
    50,063,748,296      branches                  #  146.730 M/sec                  
       738,864,975      branch-misses             #    1.48% of all branches        

     341.332239082 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.37#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.37#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.37#

               Core t (s)   Wall t (s)        (%)
       Time:      337.670      337.670      100.0
                 (ns/day)    (hour/ns)
Performance:        5.118        4.689

GROMACS reminds you: "My greatest contribution to the field of science is that I never entered it." (Colin Powell)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337679.905730      task-clock (msec)         #    1.000 CPUs utilized          
            34,811      context-switches          #    0.103 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            48,527      page-faults               #    0.144 K/sec                  
 1,110,462,657,492      cycles                    #    3.289 GHz                    
   591,795,283,628      stalled-cycles-frontend   #   53.29% frontend cycles idle   
   104,747,746,116      stalled-cycles-backend    #    9.43% backend cycles idle    
 1,831,147,585,176      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,058,248,459      branches                  #  148.242 M/sec                  
       741,635,205      branch-misses             #    1.48% of all branches        

     337.820291158 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.38#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.38#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.38#

               Core t (s)   Wall t (s)        (%)
       Time:      339.063      339.063      100.0
                 (ns/day)    (hour/ns)
Performance:        5.097        4.709

GROMACS reminds you: "The time for theory is over" (J. Hajdu)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     339099.036901      task-clock (msec)         #    1.000 CPUs utilized          
            34,949      context-switches          #    0.103 K/sec                  
                11      cpu-migrations            #    0.000 K/sec                  
            48,612      page-faults               #    0.143 K/sec                  
 1,115,397,802,832      cycles                    #    3.289 GHz                    
   596,632,966,413      stalled-cycles-frontend   #   53.49% frontend cycles idle   
   109,763,892,864      stalled-cycles-backend    #    9.84% backend cycles idle    
 1,831,154,905,279      instructions              #    1.64  insn per cycle         
                                                  #    0.33  stalled cycles per insn
    50,059,980,806      branches                  #  147.626 M/sec                  
       738,130,345      branch-misses             #    1.47% of all branches        

     339.213921221 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.39#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.39#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.39#

               Core t (s)   Wall t (s)        (%)
       Time:      338.178      338.178      100.0
                 (ns/day)    (hour/ns)
Performance:        5.110        4.696

GROMACS reminds you: "I originally implemented PME to prove that you didn't need it..." (Erik Lindahl)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     338180.083584      task-clock (msec)         #    1.000 CPUs utilized          
            34,856      context-switches          #    0.103 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            48,815      page-faults               #    0.144 K/sec                  
 1,112,369,911,839      cycles                    #    3.289 GHz                    
   593,546,710,502      stalled-cycles-frontend   #   53.36% frontend cycles idle   
   105,528,869,454      stalled-cycles-backend    #    9.49% backend cycles idle    
 1,831,141,615,749      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,058,724,637      branches                  #  148.024 M/sec                  
       739,900,371      branch-misses             #    1.48% of all branches        

     338.324713148 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.40#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.40#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.40#

               Core t (s)   Wall t (s)        (%)
       Time:      338.357      338.357      100.0
                 (ns/day)    (hour/ns)
Performance:        5.108        4.699

GROMACS reminds you: "Hmm, It *Does* Go Well With the Chicken" (Beastie Boys)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     338256.998583      task-clock (msec)         #    0.999 CPUs utilized          
            34,872      context-switches          #    0.103 K/sec                  
                12      cpu-migrations            #    0.000 K/sec                  
            48,608      page-faults               #    0.144 K/sec                  
 1,112,601,255,141      cycles                    #    3.289 GHz                    
   593,739,035,335      stalled-cycles-frontend   #   53.36% frontend cycles idle   
   103,738,004,192      stalled-cycles-backend    #    9.32% backend cycles idle    
 1,831,573,109,223      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,064,376,990      branches                  #  148.007 M/sec                  
       738,117,449      branch-misses             #    1.47% of all branches        

     338.509376568 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.41#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.41#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.41#

               Core t (s)   Wall t (s)        (%)
       Time:      337.157      337.157      100.0
                 (ns/day)    (hour/ns)
Performance:        5.126        4.682

GROMACS reminds you: "I believe in miracles cause I'm one" (The Ramones)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337177.064185      task-clock (msec)         #    1.000 CPUs utilized          
            34,750      context-switches          #    0.103 K/sec                  
                10      cpu-migrations            #    0.000 K/sec                  
            48,366      page-faults               #    0.143 K/sec                  
 1,109,086,756,165      cycles                    #    3.289 GHz                    
   590,268,013,585      stalled-cycles-frontend   #   53.22% frontend cycles idle   
   104,469,899,664      stalled-cycles-backend    #    9.42% backend cycles idle    
 1,831,057,062,811      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,058,622,034      branches                  #  148.464 M/sec                  
       740,572,813      branch-misses             #    1.48% of all branches        

     337.302927800 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.42#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.42#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.42#

               Core t (s)   Wall t (s)        (%)
       Time:      337.345      337.345      100.0
                 (ns/day)    (hour/ns)
Performance:        5.123        4.685

GROMACS reminds you: "O My God, They Killed Kenny !" (South Park)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337364.431372      task-clock (msec)         #    1.000 CPUs utilized          
            34,800      context-switches          #    0.103 K/sec                  
                14      cpu-migrations            #    0.000 K/sec                  
            49,265      page-faults               #    0.146 K/sec                  
 1,109,661,242,254      cycles                    #    3.289 GHz                    
   590,880,534,655      stalled-cycles-frontend   #   53.25% frontend cycles idle   
   104,605,598,399      stalled-cycles-backend    #    9.43% backend cycles idle    
 1,831,055,789,168      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,060,541,035      branches                  #  148.387 M/sec                  
       745,076,789      branch-misses             #    1.49% of all branches        

     337.498059247 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.43#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.43#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.43#

               Core t (s)   Wall t (s)        (%)
       Time:      335.962      335.962      100.0
                 (ns/day)    (hour/ns)
Performance:        5.144        4.666

GROMACS reminds you: "Never Get a Chance to Kick Ass" (The Amps)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     335991.733890      task-clock (msec)         #    1.000 CPUs utilized          
            34,639      context-switches          #    0.103 K/sec                  
                11      cpu-migrations            #    0.000 K/sec                  
            37,177      page-faults               #    0.111 K/sec                  
 1,105,213,931,450      cycles                    #    3.289 GHz                    
   586,385,821,231      stalled-cycles-frontend   #   53.06% frontend cycles idle   
   102,277,969,434      stalled-cycles-backend    #    9.25% backend cycles idle    
 1,830,910,990,062      instructions              #    1.66  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,054,458,284      branches                  #  148.975 M/sec                  
       741,655,353      branch-misses             #    1.48% of all branches        

     336.115305692 seconds time elapsed

                      :-) GROMACS - gmx mdrun, 2016.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.1
Executable:   /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1/bin/gmx_mpi
Data prefix:  /opt/sw/packages/intel-17.0.1/mvapich2-2.2/gromacs/2016.1
Working dir:  /scratch/mingtha/bin/gromacs/run_7
Command line:
  gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro


Back Off! I just backed up md.log to ./#md.log.44#

Running on 1 node with total 16 cores, 16 logical cores
Hardware detected on host slepner006.hpcc.rutgers.edu (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
    SIMD instructions most likely to fit this hardware: AVX_256
    SIMD instructions selected at GROMACS compile time: AVX_256

  Hardware topology: Basic

Reading file out.tpr, VERSION 2016.1 (single precision)
Using 1 MPI process
Using 1 OpenMP thread 


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up ener.edr to ./#ener.edr.44#
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

Writing final coordinates.

Back Off! I just backed up out.gro to ./#out.gro.44#

               Core t (s)   Wall t (s)        (%)
       Time:      337.145      337.145      100.0
                 (ns/day)    (hour/ns)
Performance:        5.126        4.682

GROMACS reminds you: "A computer would deserve to be called intelligent if it could deceive a human into believing that it was human." (Alan Turing)


 Performance counter stats for 'gmx_mpi mdrun -ntomp 1 -s out.tpr -c out.gro':

     337165.454772      task-clock (msec)         #    1.000 CPUs utilized          
            34,760      context-switches          #    0.103 K/sec                  
                14      cpu-migrations            #    0.000 K/sec                  
            48,369      page-faults               #    0.143 K/sec                  
 1,108,981,497,856      cycles                    #    3.289 GHz                    
   590,216,775,408      stalled-cycles-frontend   #   53.22% frontend cycles idle   
   103,488,550,539      stalled-cycles-backend    #    9.33% backend cycles idle    
 1,831,276,637,280      instructions              #    1.65  insn per cycle         
                                                  #    0.32  stalled cycles per insn
    50,061,756,113      branches                  #  148.478 M/sec                  
       737,608,240      branch-misses             #    1.47% of all branches        

     337.297253042 seconds time elapsed

